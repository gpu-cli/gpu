{
  "$schema": "https://gpu-cli.sh/schema/v1/gpu.json",
  // vLLM Models Example
  // Run Open Source LLMs with vLLM's high-performance inference engine
  //
  // Endpoints exposed:
  // - Port 8000: vLLM API (OpenAI-compatible)
  // - Port 8080: Lightweight Web UI for chatting with models
  //
  // To configure which model to run, edit models.json

  "project_id": "vllm-models",
  "provider": "runpod",

  // GPU selection - Qwen2.5-14B needs ~30GB VRAM, 48GB recommended
  // GPU type names must match RunPod API exactly
  "gpu_types": [
    { "type": "NVIDIA A40" },
    { "type": "NVIDIA L40S" },
    { "type": "NVIDIA A100 80GB PCIe" },
    { "type": "NVIDIA A100-SXM4-80GB" }
  ],
  "min_vram": 48,

  // Disk space for models (Qwen2.5-14B is ~30GB)
  "workspace_size_gb": 100,

  // Dual port forwarding - both ports accessible on localhost after gpu run
  "ports": [8000, 8080],

  // Keep-alive time after last activity (pod stops after this idle period)
  // vLLM loads models at startup, so keep-alive should account for this
  "keep_alive_minutes": 20,

  // Startup launches vLLM server + Web UI
  "startup": "bash ./startup.sh",

  // Sync conversation database back to local machine
  // WAL checkpoint happens on shutdown for clean sync
  "outputs": ["data/conversations.db"],

  // Readiness hook - wait for vLLM API to be ready before marking pod as ready
  // vLLM model loading is slow (30-120s depending on model size)
  "hooks": {
    "readiness": {
      "type": "command",
      "name": "vllm-ready",
      "run": ["curl", "-sf", "http://localhost:8000/v1/models"],
      "retry_count": 60,
      "retry_delay_secs": 3,
      "timeout_secs": 10
    }
  },

  // Environment - use RunPod's pre-cached PyTorch image for fast startup
  "environment": {
    // RunPod's PyTorch images are pre-cached on their machines (fast pull)
    // Using external images like vllm/vllm-openai:latest requires pulling
    // several GB each time, causing long startup times
    "base_image": "runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04",

    "system": {
      "apt": [
        { "name": "curl" },
        { "name": "jq" }
      ]
    },

    // Install vLLM during image build (runs once, cached for subsequent runs)
    "shell": {
      "steps": [
        {
          "run": "pip install vllm --no-cache-dir"
        }
      ]
    }
  },

  // Optional: HuggingFace token for gated models (Meta Llama, etc.)
  // Set via: gpu secret set HF_TOKEN <your-token>
  "inputs": [
    {
      "type": "secret",
      "key": "hf_token",
      "label": "HuggingFace Token",
      "description": "HuggingFace token for gated models (Llama, etc.)",
      "help": "Get your token at https://huggingface.co/settings/tokens",
      "required": false
    }
  ]
}
