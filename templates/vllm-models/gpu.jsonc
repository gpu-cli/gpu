{
  "$schema": "https://gpu-cli.sh/schema/v1/gpu.json",
  // vLLM Models Example
  // Run Open Source LLMs with vLLM's high-performance inference engine
  //
  // Endpoints exposed:
  // - Port 8000: vLLM API (OpenAI-compatible)
  // - Port 8080: Lightweight Web UI for chatting with models
  //
  // To configure which model to run, edit models.json

  "project_id": "vllm-models",
  "provider": "runpod",

  "template": {
    "name": "vLLM Models",
    "description": "High-performance LLM inference with vLLM + Web UI",
    "author": "gpu-cli"
  },

  // GPU selection - Qwen2.5-14B needs ~30GB VRAM, 48GB recommended
  // GPU type names must match RunPod API exactly
  "gpu_types": [
    { "type": "NVIDIA A40" },
    { "type": "NVIDIA L40S" },
    { "type": "NVIDIA A100 80GB PCIe" }
  ],
  "min_vram": 48,

  // Disk space for models (Qwen2.5-14B is ~30GB)
  "workspace_size_gb": 100,

  // Dual port forwarding - both ports accessible on localhost after gpu run
  "ports": [8000, 8080],

  // Keep-alive time after last activity (pod stops after this idle period)
  // vLLM loads models at startup, so keep-alive should account for this
  "keep_alive_minutes": 20,

  // Startup launches vLLM server + Web UI
  "startup": "bash ./startup.sh",

  // Sync conversation database back to local machine
  // WAL checkpoint happens on shutdown for clean sync
  "outputs": ["data/conversations.db"],

  // Readiness hook - wait for vLLM API to be ready before marking pod as ready
  // vLLM model loading is slow (30-120s depending on model size)
  "hooks": {
    "readiness": {
      "type": "command",
      "name": "vllm-ready",
      "run": ["curl", "-sf", "http://localhost:8000/v1/models"],
      "retry_count": 60,
      "retry_delay_secs": 3,
      "timeout_secs": 10
    }
  },

  // Environment - use RunPod's pre-cached PyTorch image for fast startup
  "environment": {
    // RunPod's PyTorch images are pre-cached on their machines (fast pull)
    // Using external images like vllm/vllm-openai:latest requires pulling
    // several GB each time, causing long startup times
    "base_image": "runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04",

    "system": {
      "apt": [
        { "name": "curl" },
        { "name": "jq" }
      ]
    },

    // Install vLLM during image build (runs once, cached for subsequent runs)
    "shell": {
      "steps": [
        {
          "run": "pip install vllm --no-cache-dir"
        }
      ]
    }
  },

  // Optional: HuggingFace token for gated models (Meta Llama, etc.)
  // Set via: gpu secret set HF_TOKEN <your-token>
  "inputs": [
    {
      "type": "secret",
      "key": "hf_token",
      "label": "HuggingFace Token",
      "description": "HuggingFace token for gated models (Llama, etc.)",
      "help": "Get your token at https://huggingface.co/settings/tokens",
      "required": false
    }
  ],

  // ==========================================================================
  // Serverless Deployment
  //
  // When you're ready to scale from `gpu run` to a serverless API:
  //   gpu serverless deploy
  //
  // This uses RunPod's official vLLM worker template, which exposes the
  // same OpenAI-compatible API you've been using locally â€” just now it
  // auto-scales with traffic and scales to zero when idle.
  // ==========================================================================
  "serverless": {
    "template": "vllm",

    "scaling": {
      "min_workers": 0,
      "max_workers": 3,
      "idle_timeout": 5
    },

    "volume": {
      "name": "vllm-models-cache",
      "size_gb": 100,
      "mount_path": "/workspace"
    },

    "prewarm": {
      "enabled": true,
      "mode": "cpu",
      "models": [
        "Qwen/Qwen2.5-14B-Instruct"
      ]
    },

    "runpod": {
      "ports": ["8000/http"],
      "env": {
        "MODEL_NAME": "Qwen/Qwen2.5-14B-Instruct"
      },
      "scaler_type": "queue_delay",
      "scaler_value": 4,
      "flashboot": true,
      "execution_timeout_ms": 600000
    }
  }
}
